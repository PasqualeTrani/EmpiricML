{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#empiricml","title":"EmpiricML","text":"<p>EmpiricML is an open-source Python framework designed to bring the rigor of empirical science to the Machine Learning development process. Are you tired of scattered Jupyter Notebooks and untracked experiments? EmpiricML provides a structured \"Laboratory\" environment to help you move from messy scripts to reproducible science.</p>"},{"location":"#the-philosophy-ml-as-an-empirical-science","title":"The Philosophy: ML as an Empirical Science","text":"<p>The core idea behind EmpiricML is that building a machine learning model is an iterative, scientific process. You form a hypothesis (e.g., \"Adding these specific features will decrease the error\"), and you must test it in a controlled environment. EmpiricML provides that environment through the Lab class. It encapsulates everything needed for rigorous ML experimentation:</p> <ul> <li>Train and test data management</li> <li>Cross-validation strategies</li> <li>Evaluation metrics</li> <li>Standardized criteria for comparing models</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#experiment-tracking","title":"Experiment Tracking","text":"<p>Keep a detailed ledger of every run. EmpiricML automatically stores:</p> <ul> <li>Metric performance and overfitting percentages</li> <li>Training and inference latency</li> <li>Generated predictions for downstream analysis</li> </ul>"},{"location":"#polars-native-pipelines","title":"Polars-Native Pipelines","text":"<p>Performance is at the heart of EmpiricML. Unlike scikit-learn pipelines which are NumPy-based, EmpiricML transformations utilize Polars LazyFrames. This allows for lightning-fast, memory-efficient data handling even with large datasets.</p>"},{"location":"#automated-workflows","title":"Automated Workflows","text":"<p>Stop writing boilerplate code for standard tasks. EmpiricML automates:</p> <ul> <li>Hyperparameter Optimization (HPO)</li> <li>Feature Importance calculation</li> <li>Automated Feature Selection</li> </ul>"},{"location":"#rigorous-model-comparison","title":"Rigorous Model Comparison","text":"<p>Compare experiments with statistical confidence. Define comparison criteria in your Lab class based on:</p> <p>Performance Thresholds: Does Model B outperform Model A by a significant margin? Statistical Tests: Use built-in tests to ensure your improvements aren't just noise</p> <p>EmpiricML can automatically update and store your \"Best Model\" based on these predefined rules.</p>"},{"location":"#fast-ml-baselines","title":"Fast ML Baselines","text":"<p>Go from zero to a leaderboard in seconds. With just a few lines of code, you can evaluate up to 10 baseline models (including LightGBM, XGBoost, Random Forest, MLP, and more) to establish a performance floor for your project.</p>"},{"location":"#multi-metric-evaluation","title":"Multi-Metric Evaluation","text":"<p>Evaluate models on multiple metrics simultaneously. Define a list of metrics and the Lab will track each one independently, requiring improvement on all metrics before considering a model as better. Supports per-metric minimize/maximize configuration and multi-metric HPO.</p>"},{"location":"#early-stopping","title":"Early Stopping","text":"<p>Aborts unpromising experiments early to save compute resources.</p>"},{"location":"#checkpointing","title":"Checkpointing","text":"<p>Save/Restore your <code>Lab</code> state to pause and resume work seamlessly.</p>"},{"location":"api/","title":"API Reference","text":"<p>This is the class and function reference of EmpiricML. Please select a module from the navigation menu or the table below to view its documentation.</p> Module Description <code>empml.base</code> Core abstract base classes for building other components. <code>empml.cv</code> Cross-validation splitting strategies and utilities. <code>empml.data</code> Classes for downloading and loading data. <code>empml.errors</code> Custom exceptions used throughout the framework. <code>empml.lab</code> The main experimentation framework for model development. <code>empml.metrics</code> Performance metrics for regression and classification. <code>empml.pipeline</code> Pipeline orchestration and evaluation tools. <code>empml.transformers</code> Feature engineering transformers compatible with Polars. <code>empml.wrappers</code> Wrappers for integrating Scikit-learn and PyTorch models."},{"location":"contributing/","title":"Contributing to EmpiricML","text":"<p>I appreciate your interest in contributing to EmpiricML! This guide will help you get started with reporting issues and extending the framework.</p>"},{"location":"contributing/#1-reporting-issues","title":"1. Reporting Issues","text":"<p>If you encounter any bugs, have feature requests, or want to suggest improvements, please use the GitHub Issues tracker.</p>"},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting a bug, please include:</p> <ul> <li>A clear and descriptive title.</li> <li>A detailed description of the issue.</li> <li>Steps to reproduce the bug (including code snippets if possible).</li> <li>The expected behavior vs. the actual behavior.</li> <li>Your environment details (OS, Python version, EmpiricML version).</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>For feature requests, please provide:</p> <ul> <li>A clear description of the proposed feature.</li> <li>The motivation behind the feature (why is it needed?).</li> <li>Potential implementation details (if you have ideas).</li> </ul>"},{"location":"contributing/#2-extending-classes","title":"2. Extending Classes","text":"<p>EmpiricML is built to be modular and extensible. You can easily plug in new components by inheriting from the base classes defined in <code>empml.base</code>.</p>"},{"location":"contributing/#data-downloaders","title":"Data Downloaders","text":"<p>If you need to load data from sources not currently supported (e.g., S3, REST APIs, SharePoint, etc.), you can create a custom downloader by extending the <code>DataDownloader</code> class.</p> <p>EmpiricML already includes built-in downloaders for files (CSV, Parquet, Excel), SQL databases (PostgreSQL, MySQL, MSSQL, SQLite, Oracle), and cloud warehouses (Redshift, BigQuery, Snowflake, Databricks). See <code>empml.data</code> for the full list.</p> <p>You need to implement the <code>get_data</code> method, which must return a <code>polars.LazyFrame</code>.</p>"},{"location":"contributing/#example-creating-a-rest-api-downloader","title":"Example: Creating a REST API Downloader","text":"<pre><code>import polars as pl\nfrom empml.base import DataDownloader\n\nclass RestAPIDownloader(DataDownloader):\n    \"\"\"\n    Custom downloader that fetches data from a REST API endpoint.\n    \"\"\"\n    def __init__(self, url: str, headers: dict | None = None):\n        self.url = url\n        self.headers = headers or {}\n\n    def get_data(self) -&gt; pl.LazyFrame:\n        \"\"\"\n        Fetches JSON data from a REST API and returns a Polars LazyFrame.\n        \"\"\"\n        import requests\n\n        response = requests.get(self.url, headers=self.headers)\n        response.raise_for_status()\n        data = response.json()\n\n        return pl.DataFrame(data).lazy()\n</code></pre>"},{"location":"contributing/#transformers","title":"Transformers","text":"<p>To add new data transformation logic (e.g., Clustering, dimensionality reduction, or custom feature engineering), you should extend the <code>BaseTransformer</code> class.</p> <p>You must implement two methods:</p> <ol> <li><code>fit(self, lf: pl.LazyFrame)</code>: Learns parameters from the data (if stateful).</li> <li><code>transform(self, lf: pl.LazyFrame) -&gt; pl.LazyFrame</code>: Applies the transformation.</li> </ol>"},{"location":"contributing/#example-creating-a-custom-transformer","title":"Example: Creating a Custom Transformer","text":"<pre><code>import polars as pl\nfrom empml.base import BaseTransformer\n\nclass MyTransformer(BaseTransformer):\n    \"\"\"\n    A generic transformer template.\n    Replace this with your custom transformation logic.\n    \"\"\"\n    def __init__(self, param1: str, param2: int = 10):\n        \"\"\"\n        Initialize your transformer with any parameters needed.\n\n        Args:\n            param1: Description of parameter 1\n            param2: Description of parameter 2\n        \"\"\"\n        self.param1 = param1\n        self.param2 = param2\n        # Add any attributes that will be learned during fit\n        self.fitted_value_ = None\n\n    def fit(self, lf: pl.LazyFrame):\n        \"\"\"\n        Learn parameters from the data if needed.\n        For stateless transformers, this can simply return self.\n\n        Args:\n            lf: Input Polars LazyFrame\n\n        Returns:\n            self\n        \"\"\"\n        # Example: compute and store some statistic from the data\n        # self.fitted_value_ = lf.select(pl.col(self.param1).mean()).collect().item()\n\n        return self\n\n    def transform(self, lf: pl.LazyFrame) -&gt; pl.LazyFrame:\n        \"\"\"\n        Apply the transformation to the data.\n\n        Args:\n            lf: Input Polars LazyFrame\n\n        Returns:\n            Transformed Polars LazyFrame\n        \"\"\"\n        # Example: add a new column based on your transformation logic\n        # return lf.with_columns(\n        #     (pl.col(self.param1) * self.param2).alias(\"new_feature\")\n        # )\n\n        return lf\n</code></pre> <p>By following these patterns, you can integrate virtually any custom logic into the EmpiricML pipeline.</p> <p>If you create a custom transformer or a custom downloader not included in the framework, please consider submitting a pull request to add it. Thank you!</p>"},{"location":"examples/","title":"Examples","text":"<p>This section demonstrates the core functionalities of the <code>Lab</code> class for managing machine learning experiments.</p>"},{"location":"examples/#1-initializing-lab","title":"1. Initializing Lab","text":"<p>The <code>Lab</code> class is the central orchestrator. It requires a data source, an evaluation metric, a cross-validation strategy, and comparison criteria.</p> <pre><code>from empml.lab import Lab, ComparisonCriteria\nfrom empml.data import ParquetDownloader\nfrom empml.cv import KFold\nfrom empml.metrics import RMSE\n\n# Initialize the Lab environment\nlab = Lab(\n    train_downloader=ParquetDownloader('./data/train.parquet'),\n    metric=RMSE(),\n    cv_generator=KFold(n_splits=5, random_state=7),\n    target='target_column',\n    comparison_criteria=ComparisonCriteria(\n        n_folds_threshold=2, # Experiment considered better if it improves &gt;2 folds\n        pct_threshold=0.01   # and improves metric by at least 1%\n    ),\n    minimize=True,  # True for errors (RMSE), False for scores (Accuracy)\n    row_id='row_id' # Unique identifier for each row\n)\n</code></pre>"},{"location":"examples/#2-running-a-pipeline-experiment","title":"2. Running a Pipeline Experiment","text":"<p>Create a pipeline typically involving transformers and a model wrapper, then run it.</p> <pre><code>from lightgbm import LGBMRegressor\nfrom empml.pipeline import Pipeline\nfrom empml.wrappers import SKlearnWrapper\nfrom empml.transformers import Log1pFeatures\n\n# Define a pipeline with feature engineering and model\npipeline = Pipeline([\n    ('log_features', Log1pFeatures(features=['feature1', 'feature2'])),\n    ('model', SKlearnWrapper(\n        estimator=LGBMRegressor(verbose=-1),\n        features=['feature1', 'feature2', 'feature3'],\n        target='target_column'\n    ))\n], name='LGBM_Experiment', description='LGBM with Log1p transformation')\n\n# Run the experiment\nlab.run_experiment(pipeline)\n</code></pre>"},{"location":"examples/#3-running-base-experiments","title":"3. Running Base Experiments","text":"<p>Run a suite of default baseline models (Linear Regression, KNN, Random Forest, etc.) to establish performance benchmarks.</p> <pre><code># Run a suite of baseline models\nlab.run_base_experiments(\n    features=['feature1', 'feature2', 'feature3'],\n    problem_type='regression' # or 'classification'\n)\n</code></pre>"},{"location":"examples/#4-setting-the-best-experiment","title":"4. Setting the Best Experiment","text":"<p>Mark a specific experiment as the current \"best\" to compare future experiments against.</p> <pre><code># Manually set the best experiment ID (e.g., ID 1)\nlab._set_best_experiment(experiment_id=1)\n</code></pre>"},{"location":"examples/#5-comparing-against-best-experiment","title":"5. Comparing Against Best Experiment","text":"<p>Run a new experiment and automatically compare it against the set baseline.</p> <pre><code># Define another pipeline with different parameters\npipeline_v2 = Pipeline([\n    ('model', SKlearnWrapper(\n        estimator=LGBMRegressor(n_estimators=200, verbose=-1),\n        features=['feature1', 'feature2', 'feature3'],\n        target='target_column'\n    ))\n], name='LGBM_v2')\n\n# Run and compare against the best experiment (ID 1)\nlab.run_experiment(pipeline_v2, compare_against=1)\n\n# Alternatively, use auto_mode to automatically compare against the current best experiment\n# and update the best_experiment attribute if the new pipeline performs better according to the comparison criteria\nlab._set_best_experiment(1)\nlab.run_experiment(pipeline_v2, auto_mode=True)\n</code></pre>"},{"location":"examples/#6-hyperparameter-optimization-hpo","title":"6. Hyperparameter Optimization (HPO)","text":"<p>Perform grid or random search over a managed search space.</p> <pre><code># Define hyperparameter search space\nparams_list = {\n    'n_estimators': [100, 200, 500],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [31, 64]\n}\n\n# Launch optimization\nbest_result = lab.hpo(\n    features=['feature1', 'feature2', 'feature3'],\n    params_list=params_list,\n    estimator=LGBMRegressor,\n    search_type='random',\n    num_samples=10,\n    store_preds=False\n)\n</code></pre>"},{"location":"examples/#7-permutation-feature-importance","title":"7. Permutation Feature Importance","text":"<p>Analyze which features contribute most to the model's performance.</p> <pre><code># Retrieve the best pipeline\npipeline = lab.retrieve_pipeline(experiment_id=1)\n\n# Calculate feature importance\npfi : pl.DataFrame= lab.permutation_feature_importance(\n    pipeline=pipeline,\n    features=['feature1', 'feature2', 'feature3'],\n    n_iters=5\n)\n\npfi\n</code></pre>"},{"location":"examples/#8-retrieving-predictions-error-analysis","title":"8. Retrieving Predictions &amp; Error Analysis","text":"<p>Load out-of-fold predictions to analyze where the model fails.</p> <pre><code>import polars as pl\n\n# Retrieve predictions from specific experiments\npreds = lab.retrieve_predictions(\n    experiment_ids=[1], # list of experiment IDs\n    extra_features=['date'] # Optional: add extra columns from training data\n)\n</code></pre>"},{"location":"examples/#9-multi-metric-lab","title":"9. Multi-Metric Lab","text":"<p>Evaluate models on multiple metrics simultaneously. The Lab tracks all metrics and requires improvement on all of them for a model to be considered better.</p> <pre><code>from empml.lab import Lab, ComparisonCriteria\nfrom empml.data import ParquetDownloader\nfrom empml.cv import KFold\nfrom empml.metrics import RMSE, MAE\n\n# Initialize Lab with multiple metrics\nlab = Lab(\n    train_downloader=ParquetDownloader('./data/train.parquet'),\n    metric=[RMSE(), MAE()],\n    cv_generator=KFold(n_splits=5, random_state=7),\n    target='target_column',\n    comparison_criteria=ComparisonCriteria(\n        n_folds_threshold=2,\n        pct_threshold=0.01\n    ),\n    minimize=[True, True],  # both RMSE and MAE should be minimized\n    name='multi_metric_lab'\n)\n\n# Run experiments as usual - results will contain suffixed columns\n# (cv_mean_score_1 for RMSE, cv_mean_score_2 for MAE)\nlab.run_experiment(pipeline, auto_mode=True)\n\n# View best score for each metric\nlab.show_best_score(metric_idx=0)  # best by RMSE\nlab.show_best_score(metric_idx=1)  # best by MAE\n\n# HPO with multi-metric: optimize based on a specific metric or all\nbest_result = lab.hpo(\n    features=['feature1', 'feature2', 'feature3'],\n    params_list={'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1]},\n    estimator=LGBMRegressor,\n    primary_metric_idx='all'  # or 0 for RMSE only, 1 for MAE only\n)\n</code></pre>"},{"location":"examples/#10-loading-data-from-sql-databases","title":"10. Loading Data from SQL Databases","text":"<p>EmpiricML provides built-in downloaders for SQL databases and cloud data warehouses.</p> <pre><code>from empml.data import PostgreSQLDownloader, SQLiteDownloader, SQLDownloader\n\n# PostgreSQL\npg_downloader = PostgreSQLDownloader(\n    query='SELECT * FROM my_table',\n    host='localhost',\n    user='my_user',\n    password='my_password',\n    database='my_database',\n    port=5432\n)\n\n# SQLite\nsqlite_downloader = SQLiteDownloader(\n    query='SELECT * FROM my_table',\n    path='/path/to/database.db'\n)\n\n# Generic SQL with connection URI\nsql_downloader = SQLDownloader(\n    query='SELECT * FROM my_table',\n    connection_uri='postgresql://user:password@host:5432/database'\n)\n\n# Use any downloader as train or test data source\nlab = Lab(\n    train_downloader=pg_downloader,\n    metric=RMSE(),\n    cv_generator=KFold(n_splits=5),\n    target='target_column',\n    comparison_criteria=ComparisonCriteria(n_folds_threshold=1, pct_threshold=0.01),\n    minimize=True\n)\n</code></pre>"},{"location":"examples/#11-recursive-feature-selection","title":"11. Recursive Feature Selection","text":"<p>Automatically identify and remove features that hurt model performance using permutation-based importance.</p> <pre><code>from lightgbm import LGBMRegressor\n\n# Recursively remove features with negative importance\nselected_features = lab.recursive_permutation_feature_selection(\n    estimator=LGBMRegressor(verbose=-1),\n    features=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'],\n    n_iters=5,\n    verbose=True\n)\n\n# selected_features contains only the features that contribute positively\nprint(f\"Selected features: {selected_features}\")\n</code></pre>"},{"location":"examples/#12-using-transformers-in-pipelines","title":"12. Using Transformers in Pipelines","text":"<p>EmpiricML provides a rich set of transformers for feature engineering.</p>"},{"location":"examples/#feature-interactions","title":"Feature Interactions","text":"<pre><code>from empml.transformers import InteractionFeatures\n\n# Create pairwise multiplication features\ninteractions = InteractionFeatures(\n    feature_pairs=[('feature1', 'feature2'), ('feature2', 'feature3')],\n    separator='_x_'\n)\n# Creates columns: feature1_x_feature2, feature2_x_feature3\n</code></pre>"},{"location":"examples/#frequency-encoding","title":"Frequency Encoding","text":"<pre><code>from empml.transformers import FrequencyEncoder\n\n# Encode categories by their frequency\nfreq_encoder = FrequencyEncoder(\n    features=['category_col'],\n    normalize=True,         # proportion instead of raw count\n    replace_original=False  # keep original column\n)\n</code></pre>"},{"location":"examples/#robust-scaling","title":"Robust Scaling","text":"<pre><code>from empml.transformers import RobustScaler\n\n# Scale features using median/IQR (outlier-resistant)\nscaler = RobustScaler(features=['feature1', 'feature2'])\n</code></pre>"},{"location":"examples/#quantile-binning","title":"Quantile Binning","text":"<pre><code>from empml.transformers import QuantileBinning\n\n# Discretize into quantile-based bins\nbinner = QuantileBinning(\n    features=['continuous_feature'],\n    num_bins=5,\n    labels=['very_low', 'low', 'medium', 'high', 'very_high']\n)\n</code></pre>"},{"location":"examples/#clustering-and-dimensionality-reduction","title":"Clustering and Dimensionality Reduction","text":"<pre><code>from empml.transformers import KMeansCluster, PCATransformer\n\n# Add cluster labels as a new feature\nkmeans = KMeansCluster(\n    features=['feature1', 'feature2', 'feature3'],\n    num_clusters=5,\n    new_feature='cluster_label'\n)\n\n# Reduce dimensions with PCA\npca = PCATransformer(\n    features=['feature1', 'feature2', 'feature3', 'feature4'],\n    n_components=2,\n    prefix='pc_'\n)\n</code></pre>"},{"location":"examples/#combining-transformers-in-a-pipeline","title":"Combining Transformers in a Pipeline","text":"<pre><code>from empml.pipeline import Pipeline\nfrom empml.transformers import StandardScaler, InteractionFeatures, PCATransformer\nfrom empml.wrappers import SKlearnWrapper\nfrom lightgbm import LGBMRegressor\n\n# Build a feature engineering + model pipeline\npipeline = Pipeline([\n    ('interactions', InteractionFeatures(\n        feature_pairs=[('f1', 'f2'), ('f2', 'f3')]\n    )),\n    ('scaler', StandardScaler(features=['f1', 'f2', 'f3'])),\n    ('pca', PCATransformer(features=['f1', 'f2', 'f3'], n_components=2)),\n    ('model', SKlearnWrapper(\n        estimator=LGBMRegressor(verbose=-1),\n        features=['f1', 'f2', 'f3', 'f1_x_f2', 'f2_x_f3', 'pc_0', 'pc_1'],\n        target='target'\n    ))\n], name='Advanced Pipeline')\n\nlab.run_experiment(pipeline)\n</code></pre>"},{"location":"install/","title":"Install","text":""},{"location":"install/#installation","title":"Installation","text":"<p>EmpiricML is available on PyPI and can be installed using pip:</p> <pre><code>pip install empiricml\n</code></pre> <p>Requirements: Python 3.11 or higher.</p>"},{"location":"install/#dependencies","title":"Dependencies","text":"<p>When you install EmpiricML, the following dependencies will be automatically installed:</p> <ul> <li><code>numpy&gt;=1.26.4</code></li> <li><code>pandas&gt;=2.2.2</code></li> <li><code>pyarrow&gt;=15.0.2</code></li> <li><code>polars&gt;=1.31.0</code></li> <li><code>matplotlib&gt;=3.9.0</code></li> <li><code>scikit-learn&gt;=1.7.1</code></li> <li><code>lightgbm&gt;=4.6.0</code></li> <li><code>xgboost&gt;=3.1.2</code></li> <li><code>catboost&gt;=1.2.8</code></li> <li><code>skorch&gt;=1.3.1</code></li> </ul>"},{"location":"install/#pytorch-support","title":"PyTorch Support","text":"<p>EmpiricML supports PyTorch models through the <code>TorchWrapper</code> class. However, PyTorch is not installed automatically to allow users to choose the specific version that matches their hardware configuration (CPU vs GPU, CUDA version, etc.).</p> <p>To use PyTorch models with EmpiricML:</p> <ol> <li>Install PyTorch separately by following the instructions on the official PyTorch website.</li> <li>Use the <code>TorchWrapper</code> class to integrate your PyTorch models into the EmpiricML workflow.</li> </ol>"},{"location":"api/base/","title":"empml.base","text":"Object Description <code>DataDownloader</code> Abstract class for downloading data into Polars LazyFrames. <code>CVGenerator</code> Abstract base class for cross-validation splitting strategies. <code>Metric</code> Abstract base class for performance metrics. <code>BaseTransformer</code> Abstract base class for transformers that work with Polars LazyFrames. <code>BaseEstimator</code> Abstract base class for estimators that work with Polars LazyFrames. <code>SKlearnEstimator</code> Protocol for sklearn-like estimators."},{"location":"api/base/#datadownloader","title":"DataDownloader","text":"<p>Abstract class for downloading data into Polars LazyFrames.</p>"},{"location":"api/base/#abstract-methods","title":"Abstract Methods","text":"<pre><code>@abstractmethod\ndef get_data(self) -&gt; pl.LazyFrame:\n    pass\n</code></pre>"},{"location":"api/base/#cvgenerator","title":"CVGenerator","text":"<p>Abstract base class for cross-validation splitting strategies.</p>"},{"location":"api/base/#abstract-methods_1","title":"Abstract Methods","text":"<pre><code>@abstractmethod\ndef split(self, lf : pl.LazyFrame, row_id : str) -&gt; List[Tuple[np.array]]:\n    \"\"\"Generate a list of tuple with two elements: the first one is an array containing the row indexes for the train dataset, while the second contains the row indexes for the validation dataset\"\"\"\n    pass \n</code></pre>"},{"location":"api/base/#metric","title":"Metric","text":"<p>Abstract base class for performance metrics.</p>"},{"location":"api/base/#abstract-methods_2","title":"Abstract Methods","text":"<pre><code>@abstractmethod\ndef compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    \"\"\"\n    Computes the metric, strictly requiring a Polars LazyFrame as input.\n    The final calculation executes the lazy plan to return a scalar float.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#basetransformer","title":"BaseTransformer","text":"<p>Abstract base class for transformers that work with Polars LazyFrames.</p>"},{"location":"api/base/#abstract-methods_3","title":"Abstract Methods","text":"<pre><code>@abstractmethod\ndef fit(self, lf: pl.LazyFrame):\n    \"\"\"Fit the transformer on the data.\"\"\"\n    pass\n\n@abstractmethod\ndef transform(self, lf: pl.LazyFrame) -&gt; pl.LazyFrame:\n    \"\"\"Transform the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#baseestimator","title":"BaseEstimator","text":"<p>Abstract base class for estimators that work with Polars LazyFrames.</p>"},{"location":"api/base/#abstract-methods_4","title":"Abstract Methods","text":"<pre><code>@abstractmethod\ndef fit(self, df : pl.LazyFrame):\n    \"\"\"Fit the estimator on the data.\"\"\"\n    pass\n\n@abstractmethod\ndef predict(self, df : pl.LazyFrame):\n    \"\"\"Predict by using the fitted estimator.\"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#sklearnestimator","title":"SKlearnEstimator","text":"<p>Protocol for sklearn-like estimators.</p>"},{"location":"api/base/#protocol-methods","title":"Protocol Methods","text":"<pre><code>def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -&gt; Any:\n    \"\"\"Fit the estimator.\"\"\"\n    ...\n\ndef predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Make predictions.\"\"\"\n    ...\n</code></pre>"},{"location":"api/cv/","title":"empml.cv","text":"Object Description <code>KFold</code> Standard K-Fold cross-validation with random shuffling. <code>StratifiedKFold</code> Stratified K-Fold that preserves class distribution across folds. <code>GroupKFold</code> Group K-Fold that prevents data leakage between groups. <code>LeaveOneGroupOut</code> Leave-One-Group-Out cross-validation. <code>TimeSeriesSplit</code> Time series cross-validation generator that splits data based on date ranges. <code>TrainTestSplit</code> Single train-test split with random shuffling."},{"location":"api/cv/#kfold","title":"KFold","text":"<p>Standard K-Fold cross-validation with random shuffling.</p>"},{"location":"api/cv/#methods","title":"Methods","text":"<pre><code>def __init__(self, n_splits: int = 5, random_state: int = None):\n    pass\n\ndef split(self, lf: pl.LazyFrame, row_id: str) -&gt; List[Tuple[np.array]]:\n    \"\"\"\n    Generate k-fold train/validation splits.\n\n    Parameters\n    ----------\n    lf : pl.LazyFrame\n        Input data to split.\n    row_id : str\n        Column name containing unique row identifiers.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray]]\n        List of (train_indices, validation_indices) tuples for each fold.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cv/#stratifiedkfold","title":"StratifiedKFold","text":"<p>Stratified K-Fold that preserves class distribution across folds.</p>"},{"location":"api/cv/#methods_1","title":"Methods","text":"<pre><code>def __init__(self, target_col: str, n_splits: int = 5, random_state: int = None):\n    pass\n\ndef split(self, lf: pl.LazyFrame, row_id: str) -&gt; List[Tuple[np.array]]:\n    \"\"\"\n    Generate stratified k-fold train/validation splits.\n\n    Parameters\n    ----------\n    lf : pl.LazyFrame\n        Input data to split.\n    row_id : str\n        Column name containing unique row identifiers.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray]]\n        List of (train_indices, validation_indices) tuples for each fold.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cv/#groupkfold","title":"GroupKFold","text":"<p>Group K-Fold that prevents data leakage between groups.</p>"},{"location":"api/cv/#methods_2","title":"Methods","text":"<pre><code>def __init__(self, group_col: str, n_splits: int = 5, random_state: int = None):\n    pass\n\ndef split(self, lf: pl.LazyFrame, row_id: str) -&gt; List[Tuple[np.array]]:\n    \"\"\"\n    Generate group-aware k-fold train/validation splits.\n\n    Parameters\n    ----------\n    lf : pl.LazyFrame\n        Input data to split.\n    row_id : str\n        Column name containing unique row identifiers.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray]]\n        List of (train_indices, validation_indices) tuples for each fold.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cv/#leaveonegroupout","title":"LeaveOneGroupOut","text":"<p>Leave-One-Group-Out cross-validation.</p>"},{"location":"api/cv/#methods_3","title":"Methods","text":"<pre><code>def __init__(self, group_col: str):\n    pass\n\ndef split(self, lf: pl.LazyFrame, row_id: str) -&gt; List[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Generate leave-one-group-out train/validation splits.\n\n    Parameters\n    ----------\n    lf : pl.LazyFrame\n        Input data to split.\n    row_id : str\n        Column name containing unique row identifiers.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray]]\n        List of (train_indices, validation_indices) tuples, one per group.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cv/#timeseriessplit","title":"TimeSeriesSplit","text":"<p>Time series cross-validation generator that splits data based on date ranges.</p>"},{"location":"api/cv/#methods_4","title":"Methods","text":"<pre><code>def __init__(self, windows: List[Tuple[str, str, str, str]], date_col: str):\n    \"\"\"\n    Initialize the TimeSeriesSplit cross-validator.\n\n    Parameters\n    ----------\n    windows : List[Tuple[str, str, str, str]]\n        List of date range tuples defining train/validation splits for each fold.\n        Each tuple contains (train_start, train_end, val_start, val_end).\n    date_col : str\n        Name of the date/timestamp column in the dataset.\n    \"\"\"\n    pass\n\ndef split(self, lf: pl.LazyFrame, row_id: str) -&gt; List[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Generate train/validation row indices for each fold based on date windows.\n\n    Parameters\n    ----------\n    lf : pl.LazyFrame\n        Input data containing the date column and row identifier.\n    row_id : str\n        Name of the column containing unique row identifiers.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray]]\n        List of tuples, one per fold, where each tuple contains:\n        - train_indices: numpy array of row IDs for training\n        - val_indices: numpy array of row IDs for validation\n\n    Notes\n    -----\n    - If date_col is not already datetime type, it will be automatically converted\n      from string format using polars' str.to_datetime() method.\n    - Date filtering uses inclusive start (&gt;=) and exclusive end (&lt;) boundaries.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cv/#traintestsplit","title":"TrainTestSplit","text":"<p>Single train-test split with random shuffling.</p>"},{"location":"api/cv/#methods_5","title":"Methods","text":"<pre><code>def __init__(self, test_size: float = 0.2, random_state: int = None):\n    pass\n\ndef split(self, lf: pl.LazyFrame, row_id: str) -&gt; List[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Generate single train/test split.\n\n    Parameters\n    ----------\n    lf : pl.LazyFrame\n        Input data to split.\n    row_id : str\n        Column name containing unique row identifiers.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray]]\n        Single-element list containing (train_indices, test_indices) tuple.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/data/","title":"empml.data","text":"Object Description <code>CSVDownloader</code> Class for reading a CSV file and returning a Polars LazyFrame. <code>ParquetDownloader</code> Class for reading a Parquet file and returning a Polars LazyFrame. <code>ExcelDownloader</code> Class for reading an Excel file and returning a Polars LazyFrame. <code>SQLDownloader</code> Class for reading data from any SQL database via connection URI. <code>PostgreSQLDownloader</code> Class for reading data from PostgreSQL. <code>MySQLDownloader</code> Class for reading data from MySQL. <code>MSSQLDownloader</code> Class for reading data from Microsoft SQL Server. <code>SQLiteDownloader</code> Class for reading data from a SQLite database. <code>OracleDownloader</code> Class for reading data from Oracle Database. <code>RedshiftDownloader</code> Class for reading data from Amazon Redshift. <code>BigQueryDownloader</code> Class for reading data from Google BigQuery. <code>SnowflakeDownloader</code> Class for reading data from Snowflake. <code>DatabricksDownloader</code> Class for reading data from Databricks SQL."},{"location":"api/data/#csvdownloader","title":"CSVDownloader","text":"<p>Class for reading a CSV file and returning a Polars LazyFrame.</p>"},{"location":"api/data/#methods","title":"Methods","text":"<pre><code>def __init__(self, path : str, separator : str = ';'):\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.scan_csv(self.path, separator = self.separator)\n</code></pre>"},{"location":"api/data/#parquetdownloader","title":"ParquetDownloader","text":"<p>Class for reading a Parquet file and returning a Polars LazyFrame.</p>"},{"location":"api/data/#methods_1","title":"Methods","text":"<pre><code>def __init__(self, path : str):\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.scan_parquet(self.path)\n</code></pre>"},{"location":"api/data/#exceldownloader","title":"ExcelDownloader","text":"<p>Class for reading an Excel file and returning a Polars LazyFrame.</p>"},{"location":"api/data/#methods_2","title":"Methods","text":"<pre><code>def __init__(self, path : str, sheet_name : str | None = None):\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.read_excel(self.path, sheet_name = self.sheet_name).lazy()\n</code></pre>"},{"location":"api/data/#sqldownloader","title":"SQLDownloader","text":"<p>Class for reading data from any SQL database via connection URI.</p> <p>Uses connectorx under the hood (<code>pip install connectorx</code>). Supported URI schemes: <code>postgresql://</code>, <code>mysql://</code>, <code>mssql://</code>, <code>sqlite://</code>, <code>oracle://</code>, and more.</p>"},{"location":"api/data/#methods_3","title":"Methods","text":"<pre><code>def __init__(self, query: str, connection_uri: str):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    connection_uri : str\n        Full connection URI (e.g., 'postgresql://user:pass@host:5432/db').\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.read_database_uri(self.query, self.connection_uri).lazy()\n</code></pre>"},{"location":"api/data/#postgresqldownloader","title":"PostgreSQLDownloader","text":"<p>Class for reading data from PostgreSQL and returning a Polars LazyFrame.</p> <p>Requires connectorx (<code>pip install connectorx</code>).</p>"},{"location":"api/data/#methods_4","title":"Methods","text":"<pre><code>def __init__(self, query: str, host: str, user: str, password: str, database: str, port: int = 5432):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    host : str\n        PostgreSQL server hostname.\n    user : str\n        Database username.\n    password : str\n        Database password.\n    database : str\n        Database name.\n    port : int\n        Server port (default: 5432).\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.read_database_uri(self.query, self.connection_uri).lazy()\n</code></pre>"},{"location":"api/data/#mysqldownloader","title":"MySQLDownloader","text":"<p>Class for reading data from MySQL and returning a Polars LazyFrame.</p> <p>Requires connectorx (<code>pip install connectorx</code>).</p>"},{"location":"api/data/#methods_5","title":"Methods","text":"<pre><code>def __init__(self, query: str, host: str, user: str, password: str, database: str, port: int = 3306):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    host : str\n        MySQL server hostname.\n    user : str\n        Database username.\n    password : str\n        Database password.\n    database : str\n        Database name.\n    port : int\n        Server port (default: 3306).\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.read_database_uri(self.query, self.connection_uri).lazy()\n</code></pre>"},{"location":"api/data/#mssqldownloader","title":"MSSQLDownloader","text":"<p>Class for reading data from Microsoft SQL Server and returning a Polars LazyFrame.</p> <p>Also works with Azure SQL Database and Azure Synapse Analytics since they use the same protocol. Requires connectorx (<code>pip install connectorx</code>).</p>"},{"location":"api/data/#methods_6","title":"Methods","text":"<pre><code>def __init__(self, query: str, host: str, user: str, password: str, database: str, port: int = 1433):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    host : str\n        SQL Server hostname.\n    user : str\n        Database username.\n    password : str\n        Database password.\n    database : str\n        Database name.\n    port : int\n        Server port (default: 1433).\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.read_database_uri(self.query, self.connection_uri).lazy()\n</code></pre>"},{"location":"api/data/#sqlitedownloader","title":"SQLiteDownloader","text":"<p>Class for reading data from a SQLite database and returning a Polars LazyFrame.</p> <p>Requires connectorx (<code>pip install connectorx</code>).</p>"},{"location":"api/data/#methods_7","title":"Methods","text":"<pre><code>def __init__(self, query: str, path: str):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    path : str\n        Path to the SQLite database file.\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.read_database_uri(self.query, self.connection_uri).lazy()\n</code></pre>"},{"location":"api/data/#oracledownloader","title":"OracleDownloader","text":"<p>Class for reading data from Oracle Database and returning a Polars LazyFrame.</p> <p>Requires connectorx (<code>pip install connectorx</code>).</p>"},{"location":"api/data/#methods_8","title":"Methods","text":"<pre><code>def __init__(self, query: str, host: str, user: str, password: str, database: str, port: int = 1521):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    host : str\n        Oracle server hostname.\n    user : str\n        Database username.\n    password : str\n        Database password.\n    database : str\n        Database name (service name).\n    port : int\n        Server port (default: 1521).\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.read_database_uri(self.query, self.connection_uri).lazy()\n</code></pre>"},{"location":"api/data/#redshiftdownloader","title":"RedshiftDownloader","text":"<p>Class for reading data from Amazon Redshift and returning a Polars LazyFrame.</p> <p>Requires connectorx (<code>pip install connectorx</code>).</p>"},{"location":"api/data/#methods_9","title":"Methods","text":"<pre><code>def __init__(self, query: str, host: str, user: str, password: str, database: str, port: int = 5439):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    host : str\n        Redshift cluster endpoint.\n    user : str\n        Database username.\n    password : str\n        Database password.\n    database : str\n        Database name.\n    port : int\n        Server port (default: 5439).\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    return pl.read_database_uri(self.query, self.connection_uri).lazy()\n</code></pre>"},{"location":"api/data/#bigquerydownloader","title":"BigQueryDownloader","text":"<p>Class for reading data from Google BigQuery and returning a Polars LazyFrame.</p> <p>Requires google-cloud-bigquery (<code>pip install google-cloud-bigquery</code>).</p>"},{"location":"api/data/#methods_10","title":"Methods","text":"<pre><code>def __init__(self, query: str, project_id: str, credentials_path: str | None = None):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    project_id : str\n        Google Cloud project ID.\n    credentials_path : str | None\n        Path to service account JSON credentials file.\n        If None, uses Application Default Credentials.\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    # Uses google.cloud.bigquery client and Arrow for efficient transfer\n    ...\n</code></pre>"},{"location":"api/data/#snowflakedownloader","title":"SnowflakeDownloader","text":"<p>Class for reading data from Snowflake and returning a Polars LazyFrame.</p> <p>Requires snowflake-connector-python (<code>pip install snowflake-connector-python</code>).</p>"},{"location":"api/data/#methods_11","title":"Methods","text":"<pre><code>def __init__(self, query: str, account: str, user: str, password: str, warehouse: str, database: str, schema: str):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    account : str\n        Snowflake account identifier.\n    user : str\n        Snowflake username.\n    password : str\n        Snowflake password.\n    warehouse : str\n        Snowflake warehouse name.\n    database : str\n        Snowflake database name.\n    schema : str\n        Snowflake schema name.\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    # Uses snowflake-connector-python and Arrow for efficient transfer\n    ...\n</code></pre>"},{"location":"api/data/#databricksdownloader","title":"DatabricksDownloader","text":"<p>Class for reading data from Databricks SQL and returning a Polars LazyFrame.</p> <p>Requires databricks-sql-connector (<code>pip install databricks-sql-connector</code>).</p>"},{"location":"api/data/#methods_12","title":"Methods","text":"<pre><code>def __init__(self, query: str, server_hostname: str, http_path: str, access_token: str):\n    \"\"\"\n    Parameters:\n    -----------\n    query : str\n        SQL query to execute.\n    server_hostname : str\n        Databricks workspace server hostname.\n    http_path : str\n        HTTP path for the SQL warehouse or cluster.\n    access_token : str\n        Databricks personal access token.\n    \"\"\"\n    pass\n\ndef get_data(self) -&gt; pl.LazyFrame:\n    # Uses databricks-sql-connector and Arrow for efficient transfer\n    ...\n</code></pre>"},{"location":"api/errors/","title":"empml.errors","text":"Object Description <code>RunExperimentConfigException</code> Exception raised for invalid experiment configurations. <code>RunExperimentOnTestException</code> Exception raised when errors occur during testing on the test set."},{"location":"api/errors/#runexperimentconfigexception","title":"RunExperimentConfigException","text":"<p>Exception raised for invalid experiment configurations.</p>"},{"location":"api/errors/#runexperimentontestexception","title":"RunExperimentOnTestException","text":"<p>Exception raised when errors occur during testing on the test set.</p>"},{"location":"api/lab/","title":"empml.lab","text":"Object Description <code>ComparisonCriteria</code> Statistical criteria for comparing experiment performance. <code>Lab</code> Experimentation framework for ML model development and evaluation."},{"location":"api/lab/#comparisoncriteria","title":"ComparisonCriteria","text":"<p>Statistical criteria for comparing experiment performance.</p>"},{"location":"api/lab/#attributes","title":"Attributes","text":"<ul> <li><code>n_folds_threshold</code>: Integer. Number of folds where the new model is allowed to perform worse than the baseline.</li> <li><code>pct_threshold</code>: Float (optional). Minimum percentage improvement required to consider a new model better.</li> <li><code>alpha</code>: Float (optional). Significance level for statistical testing.</li> <li><code>n_iters</code>: Integer (optional). Number of iterations for permutation testing.</li> </ul> <p>Note: You must provide either <code>pct_threshold</code> OR both <code>alpha</code> and <code>n_iters</code>. Providing both approaches raises a <code>ValueError</code>.</p>"},{"location":"api/lab/#lab","title":"Lab","text":"<p>Experimentation framework for ML model development and evaluation. Manages experiment lifecycle: data loading, CV splitting, pipeline execution, results tracking, and statistical comparison. Supports HPO, feature selection, and multi-metric evaluation.</p>"},{"location":"api/lab/#initialization","title":"Initialization","text":"<pre><code>def __init__(\n    self,\n    train_downloader: DataDownloader,\n    metric: Metric | List[Metric],\n    cv_generator: CVGenerator,\n    target: str,\n    comparison_criteria : ComparisonCriteria,\n    minimize: bool | List[bool] = True,\n    row_id: str | None = None,\n    test_downloader: DataDownloader | None = None,\n    name: str | None = None\n)\n</code></pre> <p>Parameters</p> <ul> <li><code>train_downloader</code>: Source for training data.</li> <li><code>metric</code>: Single performance metric or list of metrics for evaluation. When a list is provided, the Lab operates in multi-metric mode (see below).</li> <li><code>cv_generator</code>: Cross-validation splitting strategy.</li> <li><code>target</code>: Name of target column.</li> <li><code>comparison_criteria</code>: Statistical criteria for experiment comparison.</li> <li><code>minimize</code>: Whether to minimize metric(s) (default True). When <code>metric</code> is a list, this can be a list of bools matching the length of <code>metric</code>.</li> <li><code>row_id</code>: Column name for row identifier.</li> <li><code>test_downloader</code>: Optional test data source.</li> <li><code>name</code>: Lab identifier (auto-generated if None).</li> </ul>"},{"location":"api/lab/#multi-metric-support","title":"Multi-Metric Support","text":"<p>When a list of metrics is passed to <code>metric</code>, the Lab enters multi-metric mode:</p> <ul> <li>Suffixed columns: Results columns are suffixed with <code>_1</code>, <code>_2</code>, etc. (e.g., <code>cv_mean_score_1</code>, <code>cv_mean_score_2</code>).</li> <li>Comparison logic: When comparing experiments, the new model is considered better only if ALL metrics pass the comparison criteria.</li> <li>Primary metric: The first metric in the list is used as the primary metric by default for operations like <code>show_best_score</code>.</li> <li><code>minimize</code> list: Each metric can independently be set to minimize or maximize (e.g., <code>minimize=[True, False]</code> for RMSE + Accuracy).</li> </ul>"},{"location":"api/lab/#methods","title":"Methods","text":""},{"location":"api/lab/#run_experiment","title":"<code>run_experiment</code>","text":"<p>Execute pipeline evaluation with CV and track results.</p> <pre><code>def run_experiment(\n    self,\n    pipeline: Pipeline,\n    eval_overfitting : bool = True,      # if True, overfitting will be evaluated\n    store_preds : bool = True,           # if True, predictions will be stored\n    verbose : bool = True,               # if True, progress will be printed\n    compare_against: int | None = None,  # id of the experiment to compare against\n    auto_mode : bool = False             # if True, the best experiment will be automatically updated if all criteria are met\n)\n</code></pre>"},{"location":"api/lab/#multi_run_experiment","title":"<code>multi_run_experiment</code>","text":"<p>Execute multiple experiments sequentially.</p> <pre><code>def multi_run_experiment(\n    self,\n    pipelines: List[Pipeline],\n    eval_overfitting : bool = True,      # if True, overfitting will be evaluated\n    store_preds : bool = True,           # if True, predictions will be stored\n    verbose : bool = True,               # if True, progress will be printed\n    compare_against: int | None = None,  # id of the experiment to compare against\n    auto_mode : bool = False             # if True, the best experiment will be automatically updated if all criteria are met\n)\n</code></pre>"},{"location":"api/lab/#run_base_experiments","title":"<code>run_base_experiments</code>","text":"<p>Run suite of baseline models for quick benchmarking.</p> <pre><code>def run_base_experiments(\n    self,\n    features: str,\n    preprocess_pipe : Pipeline | None = None,  # transformer-only pipeline to apply to features\n    eval_overfitting: bool = True,             # if True, overfitting will be evaluated\n    store_preds: bool = True,                   # if True, predictions will be stored\n    verbose: bool = True,                       # if True, progress will be printed\n    compare_against: int | None = None,         # id of the experiment to compare against\n    problem_type: str = 'regression'            # 'regression' or 'classification'\n)\n</code></pre>"},{"location":"api/lab/#hpo","title":"<code>hpo</code>","text":"<p>Hyperparameter optimization via grid or random search.</p> <pre><code>def hpo(\n    self,\n    features : List[str],\n    params_list : Dict[str, List[float | int | str]],\n    estimator : SKlearnEstimator,\n    preprocessor : Pipeline | BaseTransformer = Identity(),  # transformer-only pipeline to apply to features\n    eval_overfitting : bool = True,             # if True, overfitting will be evaluated\n    store_preds : bool = True,                   # if True, predictions will be stored\n    verbose : bool = True,                       # if True, progress will be printed\n    compare_against: int | None = None,         # id of the experiment to compare against\n    search_type : str = 'grid',                  # 'grid' or 'random'\n    num_samples : int = 64,                      # number of samples for random search\n    random_state : int = 0,\n    primary_metric_idx : int | str = 'all'       # for multi-metric: which metric to optimize (0-indexed or 'all')\n)\n</code></pre> <p>Multi-Metric HPO: The <code>primary_metric_idx</code> parameter controls best-model selection:</p> <ul> <li><code>'all'</code> (default): The best model must improve on all metrics simultaneously.</li> <li><code>0</code>, <code>1</code>, etc.: Select the best model based on a single metric (0-indexed).</li> </ul>"},{"location":"api/lab/#retrieve_predictions","title":"<code>retrieve_predictions</code>","text":"<p>Load predictions from specified experiments.</p> <pre><code>def retrieve_predictions(self, experiment_ids = List[int], extra_features : List[str] = []) -&gt; pl.LazyFrame\n</code></pre>"},{"location":"api/lab/#compute_pvalue","title":"<code>compute_pvalue</code>","text":"<p>Compute permutation test p-value(s) comparing two experiments.</p> <pre><code>def compute_pvalue(\n    self,\n    experiment_ids : Tuple[int, int],\n    n_iters : int = 200,\n    extra_features: List[str] = []\n) -&gt; Union[float, List[float]]\n</code></pre> <p>Returns a single <code>float</code> for single-metric Labs, or a <code>List[float]</code> (one p-value per metric) for multi-metric Labs.</p>"},{"location":"api/lab/#permutation_feature_importance","title":"<code>permutation_feature_importance</code>","text":"<p>Compute permutation feature importance for each feature.</p> <pre><code>def permutation_feature_importance(\n    self,\n    pipeline : Pipeline,\n    features : List[str],\n    n_iters : int = 5,\n    verbose : bool = True\n) -&gt; pl.DataFrame\n</code></pre> <p>Note: For multi-metric Labs, this method uses the first metric only.</p>"},{"location":"api/lab/#recursive_permutation_feature_selection","title":"<code>recursive_permutation_feature_selection</code>","text":"<p>Recursively eliminate features with negative importance.</p> <pre><code>def recursive_permutation_feature_selection(\n    self,\n    estimator : SKlearnEstimator,\n    features : List[str],\n    preprocessor : Pipeline | BaseTransformer = Identity(),\n    n_iters : int = 5,\n    verbose : bool = True\n) -&gt; List[str]\n</code></pre>"},{"location":"api/lab/#run_experiment_on_test","title":"<code>run_experiment_on_test</code>","text":"<p>Compute performance metrics of a pipeline associated with an experiment on the test set.</p> <pre><code>def run_experiment_on_test(\n    self,\n    experiment_id : int,\n    eval_overfitting : bool = True,\n    store_preds : bool = True,\n    verbose : bool = True\n) -&gt; Dict[str, Union[float, List[float]]]\n</code></pre>"},{"location":"api/lab/#retrieve_pipeline","title":"<code>retrieve_pipeline</code>","text":"<p>Retrieve a pipeline related to an experiment.</p> <pre><code>def retrieve_pipeline(self, experiment_id : int) -&gt; Pipeline\n</code></pre>"},{"location":"api/lab/#show_best_score","title":"<code>show_best_score</code>","text":"<p>Show the stats related to the experiment with the best cv_mean_score.</p> <pre><code>def show_best_score(self, metric_idx: int | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Parameters</p> <ul> <li><code>metric_idx</code>: For multi-metric Labs, which metric to sort by (0-indexed). Defaults to the first metric.</li> </ul>"},{"location":"api/lab/#save_check_point","title":"<code>save_check_point</code>","text":"<p>Serialize current lab state to disk.</p> <pre><code>def save_check_point(self, check_point_name : str | None = None) -&gt; None\n</code></pre>"},{"location":"api/lab/#example-usage","title":"Example Usage","text":"<pre><code>from empml.lab import Lab, ComparisonCriteria\n# Assuming necessary components (downloader, metric, cv) are imported/defined\n\n# 1. Define Criteria\ncriteria = ComparisonCriteria(\n    n_folds_threshold=1,\n    pct_threshold=0.05\n)\n\n# 2. Initialize Lab (single metric)\nlab = Lab(\n    train_downloader=my_train_downloader_object,\n    metric=my_metric_object,\n    cv_generator=my_cv_object,\n    target='target_variable',\n    comparison_criteria=criteria,\n    minimize=True,\n    name='experiment_lab_01'\n)\n\n# 3. Run Base Experiments\nlab.run_base_experiments(\n    features=['feature_A', 'feature_B'],\n    problem_type='classification'\n)\n\n# 4. View Best Results\nlab.show_best_score()\n</code></pre>"},{"location":"api/lab/#multi-metric-example","title":"Multi-Metric Example","text":"<pre><code>from empml.lab import Lab, ComparisonCriteria\nfrom empml.metrics import RMSE, MAE\n\n# Initialize Lab with multiple metrics\nlab = Lab(\n    train_downloader=my_train_downloader_object,\n    metric=[RMSE(), MAE()],\n    cv_generator=my_cv_object,\n    target='target_variable',\n    comparison_criteria=ComparisonCriteria(n_folds_threshold=1, pct_threshold=0.05),\n    minimize=[True, True],  # both metrics should be minimized\n    name='multi_metric_lab'\n)\n\n# Run experiments - comparison checks ALL metrics\nlab.run_experiment(pipeline, auto_mode=True)\n\n# View best score for a specific metric\nlab.show_best_score(metric_idx=0)  # best by RMSE\nlab.show_best_score(metric_idx=1)  # best by MAE\n</code></pre>"},{"location":"api/lab/#helper-functions","title":"Helper Functions","text":""},{"location":"api/lab/#restore_check_point","title":"<code>restore_check_point</code>","text":"<p>Load saved lab state from checkpoint.</p> <pre><code>def restore_check_point(lab_name : str, check_point_name : str) -&gt; Lab\n</code></pre>"},{"location":"api/metrics/","title":"empml.metrics","text":"Object Description <code>MSE</code> Mean Squared Error. <code>RMSE</code> Root Mean Squared Error. <code>MAE</code> Mean Absolute Error. <code>MSLE</code> Mean Squared Logarithmic Error. <code>RMSLE</code> Root Mean Squared Logarithmic Error. <code>MAPE</code> Mean Absolute Percentage Error. <code>WMAE</code> Weighted Mean Absolute Error. <code>Accuracy</code> Classification accuracy. <code>Precision</code> Precision for binary classification. <code>Recall</code> Recall (Sensitivity) for binary classification. <code>F1Score</code> F1 Score for binary classification. <code>Specificity</code> Specificity (True Negative Rate) for binary classification. <code>BalancedAccuracy</code> Balanced Accuracy for binary classification. <code>ROCAUC</code> Area Under the ROC Curve for binary classification."},{"location":"api/metrics/#mse","title":"MSE","text":"<p>Mean Squared Error.</p>"},{"location":"api/metrics/#methods","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (pl.col(target) - pl.col(preds)).pow(2).mean()\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#rmse","title":"RMSE","text":"<p>Root Mean Squared Error.</p>"},{"location":"api/metrics/#methods_1","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (pl.col(target) - pl.col(preds)).pow(2).mean().sqrt()\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#mae","title":"MAE","text":"<p>Mean Absolute Error.</p>"},{"location":"api/metrics/#methods_2","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (pl.col(target) - pl.col(preds)).abs().mean()\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#msle","title":"MSLE","text":"<p>Mean Squared Logarithmic Error. Uses log1p for numerical stability.</p>"},{"location":"api/metrics/#methods_3","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (\n        (pl.col(target).log1p() - pl.col(preds).log1p()).pow(2).mean()\n    )\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#rmsle","title":"RMSLE","text":"<p>Root Mean Squared Logarithmic Error. Uses log1p for numerical stability.</p>"},{"location":"api/metrics/#methods_4","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (\n        (pl.col(target).log1p() - pl.col(preds).log1p()).pow(2).mean().sqrt()\n    )\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#mape","title":"MAPE","text":"<p>Mean Absolute Percentage Error. Returns percentage value (0-100).</p>"},{"location":"api/metrics/#methods_5","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (\n        ((pl.col(target) - pl.col(preds)).abs() / pl.col(target).abs())\n        .mean() * 100\n    )\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#wmae","title":"WMAE","text":"<p>Weighted Mean Absolute Error. Computed as sum(|errors|) / sum(target).</p>"},{"location":"api/metrics/#methods_6","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (\n        (pl.col(target) - pl.col(preds)).abs().sum() / pl.col(target).sum()\n    )\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#accuracy","title":"Accuracy","text":"<p>Classification accuracy. Proportion of correct predictions.</p>"},{"location":"api/metrics/#methods_7","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (pl.col(target) == pl.col(preds)).mean()\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#precision","title":"Precision","text":"<p>Precision for binary classification. TP / (TP + FP).</p>"},{"location":"api/metrics/#methods_8","title":"Methods","text":"<pre><code>def __init__(self, positive_class: int = 1):\n    pass\n\ndef compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (\n        ((pl.col(preds) == self.positive_class) &amp; (pl.col(target) == self.positive_class)).sum() /\n        (pl.col(preds) == self.positive_class).sum()\n    )\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#recall","title":"Recall","text":"<p>Recall (Sensitivity) for binary classification. TP / (TP + FN).</p>"},{"location":"api/metrics/#methods_9","title":"Methods","text":"<pre><code>def __init__(self, positive_class: int = 1):\n    pass\n\ndef compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (\n        ((pl.col(preds) == self.positive_class) &amp; (pl.col(target) == self.positive_class)).sum() /\n        (pl.col(target) == self.positive_class).sum()\n    )\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#f1score","title":"F1Score","text":"<p>F1 Score for binary classification. Harmonic mean of precision and recall.</p>"},{"location":"api/metrics/#methods_10","title":"Methods","text":"<pre><code>def __init__(self, positive_class: int = 1):\n    pass\n\ndef compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    tp = ((pl.col(preds) == self.positive_class) &amp; (pl.col(target) == self.positive_class)).sum()\n    pred_pos = (pl.col(preds) == self.positive_class).sum()\n    actual_pos = (pl.col(target) == self.positive_class).sum()\n\n    precision = tp / pred_pos\n    recall = tp / actual_pos\n    f1 = 2 * (precision * recall) / (precision + recall)\n\n    return lf.select(f1).collect().item()\n</code></pre>"},{"location":"api/metrics/#specificity","title":"Specificity","text":"<p>Specificity (True Negative Rate) for binary classification. TN / (TN + FP).</p>"},{"location":"api/metrics/#methods_11","title":"Methods","text":"<pre><code>def __init__(self, positive_class: int = 1):\n    pass\n\ndef compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    metric_expr = (\n        ((pl.col(preds) != self.positive_class) &amp; (pl.col(target) != self.positive_class)).sum() /\n        (pl.col(target) != self.positive_class).sum()\n    )\n    return lf.select(metric_expr).collect().item()\n</code></pre>"},{"location":"api/metrics/#balancedaccuracy","title":"BalancedAccuracy","text":"<p>Balanced Accuracy for binary classification. (Recall + Specificity) / 2.</p>"},{"location":"api/metrics/#methods_12","title":"Methods","text":"<pre><code>def __init__(self, positive_class: int = 1):\n    pass\n\ndef compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    sensitivity = (\n        ((pl.col(preds) == self.positive_class) &amp; (pl.col(target) == self.positive_class)).sum() /\n        (pl.col(target) == self.positive_class).sum()\n    )\n    specificity = (\n        ((pl.col(preds) != self.positive_class) &amp; (pl.col(target) != self.positive_class)).sum() /\n        (pl.col(target) != self.positive_class).sum()\n    )\n    balanced_acc = (sensitivity + specificity) / 2\n\n    return lf.select(balanced_acc).collect().item()\n</code></pre>"},{"location":"api/metrics/#rocauc","title":"ROCAUC","text":"<p>Area Under the ROC Curve for binary classification. Requires probability scores.</p>"},{"location":"api/metrics/#methods_13","title":"Methods","text":"<pre><code>def compute_metric(self, lf: pl.LazyFrame, target: str, preds: str) -&gt; float:\n    # Collect data and convert to numpy for sklearn computation\n    df = lf.select([pl.col(target), pl.col(preds)]).collect()\n    y_true = df[target].to_numpy()\n    y_scores = df[preds].to_numpy()\n\n    from sklearn.metrics import roc_auc_score\n    return roc_auc_score(y_true, y_scores)\n</code></pre>"},{"location":"api/pipeline/","title":"empml.pipeline","text":"Object Description <code>Pipeline</code> Custom pipeline for chaining transformers and an optional final estimator."},{"location":"api/pipeline/#pipeline","title":"Pipeline","text":"<p>Custom pipeline for chaining transformers and an optional final estimator.</p>"},{"location":"api/pipeline/#methods","title":"Methods","text":"<pre><code>def __init__(self, steps: list[tuple[str, Union[BaseTransformer, BaseEstimator, 'Pipeline']]], name : str = '', description : str = ''):\n    \"\"\"\n    Parameters:\n    -----------\n    steps : list of tuples\n        List of (name, transformer/estimator/pipeline) tuples in the order they should be applied.\n        If the last step is an estimator, the pipeline will support predict().\n        If all steps are transformers (or pipelines acting as transformers), the pipeline \n        will support transform().\n    \"\"\"\n    pass\n\ndef fit(self, lf: pl.LazyFrame, **fit_params):\n    \"\"\"\n    Fit all transformers and the final estimator (if present).\n\n    Parameters:\n    -----------\n    lf : pl.LazyFrame\n        Training data\n    **fit_params : dict\n        Parameters to pass to the final estimator's fit method\n    \"\"\"\n    pass\n\ndef transform(self, lf: pl.LazyFrame) -&gt; pl.LazyFrame:\n    \"\"\"\n    Apply all transformers sequentially.\n    Only available for transformer-only pipelines.\n\n    Parameters:\n    -----------\n    lf : pl.LazyFrame\n        Data to transform\n\n    Returns:\n    --------\n    pl.LazyFrame\n        Transformed data\n    \"\"\"\n    if not self._is_transformer_only:\n        raise ValueError(\n            \"transform() is only available for transformer-only pipelines. \"\n            \"This pipeline has an estimator as the final step. Use predict() instead.\"\n        )\n\n    lf_transformed = lf\n    for name, step in self.steps:\n        if isinstance(step, Pipeline):\n            lf_transformed = step.transform(lf_transformed)\n        else:\n            lf_transformed = step.transform(lf_transformed)\n\n    return lf_transformed\n\ndef fit_transform(self, lf: pl.LazyFrame) -&gt; pl.LazyFrame:\n    \"\"\"\n    Fit and transform in one step.\n    Only available for transformer-only pipelines.\n    \"\"\"\n    pass\n\ndef predict(self, lf: pl.LazyFrame) -&gt; np.ndarray:\n    \"\"\"\n    Apply all transformers and predict with the final estimator.\n    Only available for pipelines with an estimator as the final step.\n\n    Parameters:\n    -----------\n    lf : pl.LazyFrame\n        Data to predict on\n\n    Returns:\n    --------\n    np.ndarray\n        Predictions\n    \"\"\"\n    pass\n\ndef fit_predict(self, lf: pl.LazyFrame, **fit_params) -&gt; np.ndarray:\n    \"\"\"\n    Fit the pipeline and return predictions on the same data.\n    Only available for pipelines with an estimator.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pipeline/#example-usage-transformer-only","title":"Example Usage (Transformer-only)","text":"<pre><code>from empml.pipeline import Pipeline\nfrom empml.transformers import AvgFeatures, MaxFeatures\nimport polars as pl\n\n# Create a sample LazyFrame\ndata = pl.DataFrame({\n    \"f1\": [1, 2, 3],\n    \"f2\": [4, 5, 6],\n    \"target\": [0, 1, 0]\n}).lazy()\n\n# Define a pipeline with only transformers\npreprocessing_pipeline = Pipeline(steps=[\n    ('avg_f1_f2', AvgFeatures(features=['f1', 'f2'], new_feature='avg_f1_f2')),\n    ('max_f1_f2', MaxFeatures(features=['f1', 'f2'], new_feature='max_f1_f2'))\n], name='Preprocessing', description='Feature Engineering Pipeline')\n\n# transform data\nprocessed_data = preprocessing_pipeline.fit_transform(data)\n\n# Collect results\nprocessed_data.collect()\n</code></pre>"},{"location":"api/transformers/","title":"empml.transformers","text":"Object Description <code>Identity</code> Pass-through transformer. <code>AvgFeatures</code> Compute mean across multiple features row-wise. <code>MaxFeatures</code> Compute max across multiple features row-wise. <code>MinFeatures</code> Compute min across multiple features row-wise. <code>StdFeatures</code> Compute standard deviation across multiple features row-wise. <code>MedianFeatures</code> Compute median across multiple features row-wise. <code>ModuleFeatures</code> Compute Euclidean norm (module) of two features. <code>InteractionFeatures</code> Create pairwise multiplication features from feature pairs. <code>MeanTargetEncoder</code> Target encoding using mean of target variable. <code>StdTargetEncoder</code> Target encoding using standard deviation of target variable. <code>MaxTargetEncoder</code> Target encoding using max of target variable. <code>MinTargetEncoder</code> Target encoding using min of target variable. <code>MedianTargetEncoder</code> Target encoding using median of target variable. <code>KurtTargetEncoder</code> Target encoding using kurtosis of target variable. <code>SkewTargetEncoder</code> Target encoding using skewness of target variable. <code>OrdinalEncoder</code> Encode categorical features as ordinal integers. <code>DummyEncoder</code> One-hot encode categorical features. <code>FrequencyEncoder</code> Encode categorical features by their frequency or proportion. <code>StandardScaler</code> Standardize features by removing the mean and scaling to unit variance. <code>MinMaxScaler</code> Scale features to [0, 1] range using min-max normalization. <code>RobustScaler</code> Scale features using median and interquartile range (IQR). <code>Log1pFeatures</code> Apply log(1+x) transformation. <code>Expm1Features</code> Apply exp(x-1) transformation. <code>PowerFeatures</code> Apply power transformation. <code>InverseFeatures</code> Apply inverse transformation (1/x). <code>QuantileBinning</code> Discretize continuous features into quantile-based bins. <code>RankFeatures</code> Convert features to percentile rank based on training distribution. <code>SimpleImputer</code> Impute missing values using mean or median. <code>FillNulls</code> Fill null and NaN values with a constant. <code>GenerateLags</code> Generate lagged features for time series data. <code>KMeansCluster</code> Assign cluster labels using KMeans on selected features. <code>PCATransformer</code> Reduce dimensionality using Principal Component Analysis."},{"location":"api/transformers/#identity","title":"Identity","text":"<p>Pass-through transformer that returns data unchanged. No parameters required.</p>"},{"location":"api/transformers/#avgfeatures","title":"AvgFeatures","text":"<p>Compute mean across multiple features row-wise.</p>"},{"location":"api/transformers/#methods","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], new_feature: str):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to average.\n    new_feature : str\n        Name of the output column.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#maxfeatures","title":"MaxFeatures","text":"<p>Compute max across multiple features row-wise.</p>"},{"location":"api/transformers/#methods_1","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], new_feature: str):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to compute max over.\n    new_feature : str\n        Name of the output column.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#minfeatures","title":"MinFeatures","text":"<p>Compute min across multiple features row-wise.</p>"},{"location":"api/transformers/#methods_2","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], new_feature: str):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to compute min over.\n    new_feature : str\n        Name of the output column.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#stdfeatures","title":"StdFeatures","text":"<p>Compute standard deviation across multiple features row-wise.</p>"},{"location":"api/transformers/#methods_3","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], new_feature: str):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to compute std over.\n    new_feature : str\n        Name of the output column.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#medianfeatures","title":"MedianFeatures","text":"<p>Compute median across multiple features row-wise.</p>"},{"location":"api/transformers/#methods_4","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], new_feature: str):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to compute median over.\n    new_feature : str\n        Name of the output column.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#modulefeatures","title":"ModuleFeatures","text":"<p>Compute Euclidean norm (module) of two features: <code>sqrt(f1^2 + f2^2)</code>.</p>"},{"location":"api/transformers/#methods_5","title":"Methods","text":"<pre><code>def __init__(self, features: tuple[str, str], new_feature: str):\n    \"\"\"\n    Parameters:\n    -----------\n    features : tuple[str, str]\n        Tuple of two column names.\n    new_feature : str\n        Name of the output column.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#interactionfeatures","title":"InteractionFeatures","text":"<p>Create pairwise multiplication features from feature pairs. For each pair <code>(f1, f2)</code>, creates a column <code>f1 * f2</code>.</p>"},{"location":"api/transformers/#methods_6","title":"Methods","text":"<pre><code>def __init__(self, feature_pairs: list[tuple[str, str]], separator: str = '_x_'):\n    \"\"\"\n    Parameters:\n    -----------\n    feature_pairs : list[tuple[str, str]]\n        List of (col1, col2) tuples to multiply.\n    separator : str\n        String between feature names in the output column name (default: '_x_').\n        Output column name: '{col1}{separator}{col2}'.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#meantargetencoder","title":"MeanTargetEncoder","text":"<p>Encode categorical features with the mean of a target variable. Unseen categories during transform are filled with the global mean.</p>"},{"location":"api/transformers/#methods_7","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    encoder_col: str,\n    prefix: str = 'mean_',\n    suffix: str = '_encoded',\n    replace_original: bool = False\n):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Categorical columns to encode.\n    encoder_col : str\n        Target column to aggregate.\n    prefix : str\n        Prefix for encoded column names (default: 'mean_').\n    suffix : str\n        Suffix for encoded column names (default: '_encoded').\n    replace_original : bool\n        If True, drop original columns and use their names\n        for encoded columns, ignoring prefix/suffix (default: False).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#stdtargetencoder","title":"StdTargetEncoder","text":"<p>Encode categorical features with the standard deviation of a target variable. Same interface as <code>MeanTargetEncoder</code>.</p>"},{"location":"api/transformers/#methods_8","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    encoder_col: str,\n    prefix: str = 'std_',\n    suffix: str = '_encoded',\n    replace_original: bool = False\n):\n    pass\n</code></pre>"},{"location":"api/transformers/#maxtargetencoder","title":"MaxTargetEncoder","text":"<p>Encode categorical features with the max of a target variable. Same interface as <code>MeanTargetEncoder</code>.</p>"},{"location":"api/transformers/#methods_9","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    encoder_col: str,\n    prefix: str = 'max_',\n    suffix: str = '_encoded',\n    replace_original: bool = False\n):\n    pass\n</code></pre>"},{"location":"api/transformers/#mintargetencoder","title":"MinTargetEncoder","text":"<p>Encode categorical features with the min of a target variable. Same interface as <code>MeanTargetEncoder</code>.</p>"},{"location":"api/transformers/#methods_10","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    encoder_col: str,\n    prefix: str = 'min_',\n    suffix: str = '_encoded',\n    replace_original: bool = False\n):\n    pass\n</code></pre>"},{"location":"api/transformers/#mediantargetencoder","title":"MedianTargetEncoder","text":"<p>Encode categorical features with the median of a target variable. Same interface as <code>MeanTargetEncoder</code>.</p>"},{"location":"api/transformers/#methods_11","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    encoder_col: str,\n    prefix: str = 'median_',\n    suffix: str = '_encoded',\n    replace_original: bool = False\n):\n    pass\n</code></pre>"},{"location":"api/transformers/#kurttargetencoder","title":"KurtTargetEncoder","text":"<p>Encode categorical features with the kurtosis of a target variable. Same interface as <code>MeanTargetEncoder</code>.</p>"},{"location":"api/transformers/#methods_12","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    encoder_col: str,\n    prefix: str = 'kurt_',\n    suffix: str = '_encoded',\n    replace_original: bool = False\n):\n    pass\n</code></pre>"},{"location":"api/transformers/#skewtargetencoder","title":"SkewTargetEncoder","text":"<p>Encode categorical features with the skewness of a target variable. Same interface as <code>MeanTargetEncoder</code>.</p>"},{"location":"api/transformers/#methods_13","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    encoder_col: str,\n    prefix: str = 'skew_',\n    suffix: str = '_encoded',\n    replace_original: bool = False\n):\n    pass\n</code></pre>"},{"location":"api/transformers/#ordinalencoder","title":"OrdinalEncoder","text":"<p>Encode categorical features with ordinal integers based on sorted order. Null values are encoded as <code>-99</code>, unknown categories (not seen during fit) as <code>-9999</code>.</p>"},{"location":"api/transformers/#methods_14","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    suffix: str = '_ordinal_encoded',\n    replace_original: bool = False\n):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Categorical columns to encode.\n    suffix : str\n        Suffix for encoded column names (default: '_ordinal_encoded').\n    replace_original : bool\n        If True, drop original columns and use their names\n        for encoded columns, ignoring suffix (default: False).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#dummyencoder","title":"DummyEncoder","text":"<p>One-hot encode categorical features. Creates binary columns for each category, plus dedicated columns for null and unknown values.</p>"},{"location":"api/transformers/#methods_15","title":"Methods","text":"<pre><code>def __init__(self, features: list[str]):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Categorical columns to encode.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#frequencyencoder","title":"FrequencyEncoder","text":"<p>Encode categorical features by their frequency (count) or proportion. Unseen categories during transform are filled with 0.</p>"},{"location":"api/transformers/#methods_16","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    normalize: bool = True,\n    prefix: str = 'freq_',\n    suffix: str = '_encoded',\n    replace_original: bool = False\n):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Categorical columns to encode.\n    normalize : bool\n        If True, encode as proportion (0-1); if False, as raw count (default: True).\n    prefix : str\n        Prefix for encoded column names (default: 'freq_').\n    suffix : str\n        Suffix for encoded column names (default: '_encoded').\n    replace_original : bool\n        If True, drop original columns and use their names\n        for encoded columns, ignoring prefix/suffix (default: False).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#standardscaler","title":"StandardScaler","text":"<p>Standardize features by removing the mean and scaling to unit variance (z-score normalization). Handles zero standard deviation by returning 0.</p>"},{"location":"api/transformers/#methods_17","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], suffix: str = ''):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to standardize.\n    suffix : str\n        Suffix for scaled column names (default: '' overwrites original).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#minmaxscaler","title":"MinMaxScaler","text":"<p>Scale features to [0, 1] range using min-max normalization. Handles zero range by returning 0.</p>"},{"location":"api/transformers/#methods_18","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], suffix: str = ''):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to scale.\n    suffix : str\n        Suffix for scaled column names (default: '' overwrites original).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#robustscaler","title":"RobustScaler","text":"<p>Scale features using median and interquartile range (IQR): <code>(x - median) / (Q75 - Q25)</code>. Less sensitive to outliers than <code>StandardScaler</code>. Handles zero IQR by returning 0.</p>"},{"location":"api/transformers/#methods_19","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], suffix: str = ''):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to scale.\n    suffix : str\n        Suffix for scaled column names (default: '' overwrites original).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#log1pfeatures","title":"Log1pFeatures","text":"<p>Apply <code>log(1+x)</code> transformation to features.</p>"},{"location":"api/transformers/#methods_20","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], suffix: str = ''):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to transform.\n    suffix : str\n        Suffix for output column names (default: '' overwrites original).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#expm1features","title":"Expm1Features","text":"<p>Apply <code>exp(x-1)</code> transformation to features.</p>"},{"location":"api/transformers/#methods_21","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], suffix: str = ''):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to transform.\n    suffix : str\n        Suffix for output column names (default: '' overwrites original).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#powerfeatures","title":"PowerFeatures","text":"<p>Raise features to a specified power.</p>"},{"location":"api/transformers/#methods_22","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], suffix: str = '', power: float = 2):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to transform.\n    suffix : str\n        Suffix for output column names (default: '' overwrites original).\n    power : float\n        Exponent for power transformation (default: 2).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#inversefeatures","title":"InverseFeatures","text":"<p>Apply inverse (<code>1/x</code>) transformation to features.</p>"},{"location":"api/transformers/#methods_23","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], suffix: str = ''):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to transform.\n    suffix : str\n        Suffix for output column names (default: '' overwrites original).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#quantilebinning","title":"QuantileBinning","text":"<p>Discretize continuous features into quantile-based bins. During <code>fit</code>, computes quantile bin edges. During <code>transform</code>, assigns bin indices (0 to num_bins-1).</p>"},{"location":"api/transformers/#methods_24","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    num_bins: int = 10,\n    suffix: str = '_qbin',\n    labels: list[str] | None = None\n):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to discretize.\n    num_bins : int\n        Number of quantile bins (default: 10).\n    suffix : str\n        Suffix for binned column names (default: '_qbin').\n    labels : list[str] | None\n        Optional string labels for bins; length must equal num_bins.\n\n    Raises:\n    -------\n    ValueError\n        If labels length does not match num_bins.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#rankfeatures","title":"RankFeatures","text":"<p>Convert features to percentile rank [0, 1] based on the training distribution.</p>"},{"location":"api/transformers/#methods_25","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    suffix: str = '_rank',\n    method: Literal['average', 'min', 'max', 'dense'] = 'average'\n):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to rank.\n    suffix : str\n        Suffix for ranked column names (default: '_rank').\n    method : str\n        Ranking method (default: 'average'):\n        - 'average': average of min and max rank positions\n        - 'min': lowest rank position\n        - 'max': highest rank position\n        - 'dense': like 'min' but ranks always increase by 1\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#simpleimputer","title":"SimpleImputer","text":"<p>Impute missing values (null and NaN) using mean or median strategy.</p>"},{"location":"api/transformers/#methods_26","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], strategy: str = 'mean'):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to impute.\n    strategy : str\n        Imputation strategy: 'mean' or 'median' (default: 'mean').\n\n    Raises:\n    -------\n    ValueError\n        If strategy is not 'mean' or 'median'.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#fillnulls","title":"FillNulls","text":"<p>Fill null and NaN values with a constant.</p>"},{"location":"api/transformers/#methods_27","title":"Methods","text":"<pre><code>def __init__(self, features: list[str], value: float = -9999):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Columns to fill.\n    value : float\n        Constant to use for filling (default: -9999).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#generatelags","title":"GenerateLags","text":"<p>Generate lagged features for time series data by joining shifted dates.</p>"},{"location":"api/transformers/#methods_28","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    ts_index: str,\n    date_col: str,\n    lag_col: str,\n    lag_frequency: str = 'days',\n    lag_min: int = 1,\n    lag_max: int = 1,\n    lag_step: int = 1\n):\n    \"\"\"\n    Parameters:\n    -----------\n    ts_index : str\n        Column for time series identifier (e.g., entity ID).\n    date_col : str\n        Date/time column (must be Polars Date or Datetime type).\n    lag_col : str\n        Column to lag.\n    lag_frequency : str\n        Time unit for lags (default: 'days').\n        Options: 'weeks', 'days', 'hours', 'minutes', 'seconds',\n        'milliseconds', 'microseconds', 'nanoseconds'.\n    lag_min : int\n        Minimum lag period (default: 1).\n    lag_max : int\n        Maximum lag period (default: 1).\n    lag_step : int\n        Step size between lags (default: 1).\n\n    Raises:\n    -------\n    ValueError\n        If lag_frequency is not a valid time unit.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#kmeanscluster","title":"KMeansCluster","text":"<p>Assign cluster labels using KMeans on selected features. Uses scikit-learn's KMeans internally. Missing values are imputed with column means before clustering.</p>"},{"location":"api/transformers/#methods_29","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    num_clusters: int = 8,\n    new_feature: str = 'kmeans_cluster',\n    random_state: int = 42\n):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Numeric columns to use for clustering.\n    num_clusters : int\n        Number of clusters (default: 8).\n    new_feature : str\n        Name of the output cluster column (default: 'kmeans_cluster').\n    random_state : int\n        Random seed for reproducibility (default: 42).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/transformers/#pcatransformer","title":"PCATransformer","text":"<p>Reduce dimensionality using Principal Component Analysis. Uses scikit-learn's PCA internally. Missing values are imputed with column means before PCA.</p>"},{"location":"api/transformers/#methods_30","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    features: list[str],\n    n_components: int = 2,\n    prefix: str = 'pc_'\n):\n    \"\"\"\n    Parameters:\n    -----------\n    features : list[str]\n        Numeric columns to use for PCA.\n    n_components : int\n        Number of principal components to keep (default: 2).\n    prefix : str\n        Prefix for output column names (default: 'pc_').\n        Columns are named '{prefix}0', '{prefix}1', etc.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/wrappers/","title":"empml.wrappers","text":"Object Description <code>SKlearnWrapper</code> Wraps sklearn-like estimators for Polars LazyFrames. <code>TorchWrapper</code> Wrapper for PyTorch modules compatible with Polars LazyFrames."},{"location":"api/wrappers/#sklearnwrapper","title":"SKlearnWrapper","text":"<p>Wraps sklearn-like estimators for Polars LazyFrames.</p>"},{"location":"api/wrappers/#methods","title":"Methods","text":"<pre><code>def __init__(self, estimator: SKlearnEstimator, features: List[str], target: str):\n    pass\n\ndef fit(self, lf: pl.LazyFrame, **fit_kwargs):\n    \"\"\"Fit the wrapped estimator using Polars LazyFrame.\"\"\"\n    X = lf.select(self.features).collect().to_numpy()\n    y = lf.select(self.target).collect().to_series().to_numpy()\n\n    self.estimator.fit(X, y, **fit_kwargs)\n    return self\n\ndef predict(self, lf: pl.LazyFrame) -&gt; np.ndarray:\n    \"\"\"Predict using the wrapped estimator with Polars LazyFrame.\"\"\"\n    X = lf.select(self.features).collect().to_numpy()\n    return self.estimator.predict(X)\n\ndef predict_proba(self, lf: pl.LazyFrame) -&gt; np.ndarray:\n    \"\"\"\n    Predict class probabilities using the wrapped estimator with Polars LazyFrame.\n\n    Only available if the wrapped estimator has a predict_proba method.\n    \"\"\"\n    X = lf.select(self.features).collect().to_numpy()\n    return self.estimator.predict_proba(X)\n</code></pre>"},{"location":"api/wrappers/#torchwrapper","title":"TorchWrapper","text":"<p>Wrapper for PyTorch modules compatible with Polars LazyFrames.</p>"},{"location":"api/wrappers/#methods_1","title":"Methods","text":"<pre><code>def __init__(\n    self,\n    module: type[nn.Module],\n    features: List[str],\n    target: str,\n    task: str = 'regression',\n    # Module architecture parameters\n    input_dim: Optional[int] = None,\n    hidden_layers: Optional[List[int]] = None,\n    output_dim: Optional[int] = None,\n    # Skorch training parameters\n    max_epochs: int = 10,\n    lr: float = 0.01,\n    batch_size: int = 128,\n    optimizer: Any = None,\n    criterion: Any = None,\n    # Skorch regularization &amp; training\n    train_split: Any = None,\n    callbacks: Optional[List] = None,\n    warm_start: bool = False,\n    verbose: int = 0,\n    # Skorch device &amp; performance\n    device: str = 'cpu',\n    # Skorch iterator settings\n    iterator_train: Any = None,\n    iterator_train__shuffle: bool = True,\n    iterator_train__num_workers: int = 0,\n    iterator_valid: Any = None,\n    iterator_valid__shuffle: bool = False,\n    # Additional skorch parameters (passed as **kwargs to NeuralNet)\n    **kwargs\n):\n    pass\n\ndef fit(self, lf: pl.LazyFrame, **fit_kwargs):\n    \"\"\"\n    Fit the wrapped PyTorch model using Polars LazyFrame.\n\n    Automatically converts data to float32 as required by PyTorch and creates\n    the skorch estimator on first call.\n    \"\"\"\n    pass\n\ndef predict(self, lf: pl.LazyFrame) -&gt; np.ndarray:\n    \"\"\"\n    Predict using the wrapped PyTorch model with Polars LazyFrame.\n\n    Automatically converts input to float32 and flattens output for regressors.\n    \"\"\"\n    pass\n\ndef predict_proba(self, lf: pl.LazyFrame) -&gt; np.ndarray:\n    \"\"\"\n    Predict class probabilities using the wrapped PyTorch classifier.\n\n    Only available for classification tasks. Automatically converts input to float32.\n    \"\"\"\n    pass\n</code></pre>"}]}